<html lang="en">
  <page-title title="Dynamic patrol behaviour in stealth games" />
  <link
    rel="syndication"
    href="https://www.reddit.com/r/IndieDev/comments/14nu6zf/i_made_a_potentialfield_based_algorithm_for/"
  />
  <blog-page header-select="/notes/">
    <!-- prettier-ignore -->
    <markdown>
      <blog-header
        title="Dynamic patrol behaviour in stealth games with Markov chains"
        :heroimgsrc="url('hero.jpg')" />

      <blog-post-info date="1 Jul 2023" read-mins="8" />

      <tag-row>
        <tag>algo</tag>
        <tag>games</tag>
      </tag-row>

      Hi, this post is about a game AI algorithm for stealth games.
      
      But first, here‚Äôs a preview demo! Full demo at the end of this post. In between, I‚Äôll explain the background, the process, and the results!

      <dynamic-patrol-demo :map="`
        ###################
        #....#....#.......#
        #....#....#.......#
        #....#....#.......#
        ##.####.###.......#
        #.................#
        #.................#
        ##.####.###.......#
        #...#....#####..###
        #...#....#####..###
        #...#...G#####..###
        ##########.......##
        ##########.......##
        ##########.......##
        ##########.......##
        ###################
      `"/>

      ## Background

      I enjoy stealth mechanics in video games. However, I feel like the genre has become formulaic. Nowadays, we have standardised light, shadow, and noise mechanics. We almost always get discrete levels of alertness where on one end NPCs have wallhacks while on the other, NPCs have amnesia.

      <blog-media
        :src="url('sc.jpg')"
        alt="Screenshot of Splinter Cell"
        caption="Screenshot of Splinter Cell: Chaos Theory from mobygames.com. This game is good." />

      The most immersion-breaking moment for me was when you get spotted, the subsequent investigation consists solely of staring at the ground where you were last seen. Like when you get spotted at an entrance to a room, guards will just stare at the doorway. *Why not check inside the room?* Unless the room was specifically coded as part of a patrol route, guards will never investigate there even though they just saw you entering the room.

      <box-note>Yeah, I know, smart game AI is not deemed ‚Äúfun‚Äù by game designers. But a challenging game is fun! Whatever, I just wanted to write a relatable intro for this prototype that I made for fun.</box-note>

      What was lacking in stealth game AI is inference - the ability to infer that when a target enters a room, then they subsequently must be inside the room.
      
      <!-- textlint-disable -->

      Or more generally, if a target is at position *X* at time *t*, then it will be in one of the adjacent positions *Y<sub>1</sub>*, *Y<sub>2</sub>*, *Y<sub>3</sub>*, &hellip; *Y<sub>n</sub>* at time *t+1*.

      <!-- textlint-enable -->

      ## A room and a hallway

      As an example, here are a room and a hallway with a doorway in between, modelled as a [graph](https://en.wikipedia.org/wiki/Graph):

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph.png')"
          alt="Node graph representing a room node, a doorway node, and hallway nodes"
          spec="300" />
      </div>

      If we assign a number to each node representing the probability that the target (the player) is there, we can start making inferences of where the target could be at later times.

      Let‚Äôs say the target was just seen in room *R*, so node *R* will be assigned a probability of **1.0**.
      
      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-r1.png')"
          alt="Node graph representing the room node with 1.0, the doorway node with 0.0, and hallway nodes with 0.0"
          spec="300" />
      </div>

      After this, the target will either stay in the room or exit the room. We don‚Äôt know the chance of either happening so we can just assume equal chances. The graph is recalculated to reflect this by equally distributing the **1.0** from *R* to itself (‚Äústay‚Äù) and its adjacent node *D* (‚Äúexit‚Äù), giving them **0.5** each.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-r1d1.png')"
          alt="Node graph representing the room node with 0.5, the doorway node with 0.5, and hallway nodes with 0.0"
          spec="300" />
      </div>

      Now there is 50% probability that the target is in the room, and 50% in the doorway.
      
      This next iteration is tricky, but it‚Äôs all calculated the same. We just need to calculate the distribution from each node *in parallel*, like so:

      <div class="small-diagram-row">
        <blog-media
          :src="url('room-graph-r1d1t.png')"
          alt="Node graph representing the distribution from previous state"
          spec="300"
          caption="Split the probabilities per node to each possible path (including itself)." />
        <blog-media
          :src="url('room-graph-r5d5h2.png')"
          alt="Node graph representing the room node with 0.42, the doorway node with 0.42, and the hallway node H1 directly next to the doorway with 0.16"
          spec="300"
          caption="Then sum up the values that arrived in each node." />
      </div>

      After some time, we will get a picture of where the target is likely to be and a smarter game AI can utilise this to send guards on a more realistic investigation route.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-final.png')"
          alt="Node graph representing the room node with 0.35, the doorway node with 0.39, hallway nodes H0 and H2 with 0.04, and H1 with 0.18"
          spec="300"
          caption="The state after some time" />
      </div>

      What I‚Äôve just described is some generalisation of a **[Markov chain](https://en.wikipedia.org/wiki/Markov_chain)**. Well, it‚Äôs not exactly accurate to call it that since the Markov chain is just one part of the algorithm. You‚Äôll see why in the next section.

      <box-note><strong>Disclaimer:</strong> I wasn‚Äôt thinking about Markov chains while developing this algorithm. The first version was based on crude counting and was more like a potential field of arbitrary scale. The Markov chain concept helped me make the calculations more accurate and the numbers behave more like probabilities.</box-note>

      ## Observer effect

      Suppose a guard did come to investigate the nearest highest probability node (the doorway *D*). Coming from the south, the guard just saw the doorway and the immediate hallway in their field of vision - There are two possibilities: Either **(1)** they saw nothing, or **(2)** they saw *the target*.

      In case **(1)** where the guard saw nothing, we need to update the seen nodes according to the guard‚Äôs a posteriori observation.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('observe-none.png')"
          alt="Node graph with observed nodes having 0.0"
          spec="300"
          caption="After observation" />
      </div>

      The nodes that were seen having no target at their locations are forced to a probability of **0.0**, because if you think about it, that makes sense. The remaining nodes are scaled so that they still add up to **1.0** (This is an invariant in any case).

      To illustrate, here‚Äôs the previous state: **H0=0.04** and **R=0.35** before observation. Then after the observation, we need these remaining nodes to have a total of **1.0** while keeping the proportions. One way to do that is to *divide each value by the sum of all values*. So **H0=0.04/0.39** and **R=0.35/0.39**, which evaluate to **H0=~0.103** and **R=~0.897** respectively.

      Here's a table that details each step described above.

      <div class="style-hax"></div>

      | Node | Prior<br/>probabilities | Values after<br/>observation | Final values<br/>after rescaling |
      |------|-------|-------------|----------|
      | R    | 0.35  | 0.35        | **0.9**  |
      | D    | 0.39  | **0**       | 0        |
      | H0   | 0.04  | 0.04        | **0.1**  |
      | H1   | 0.18  | **0**       | 0        |
      | H2   | 0.04  | **0**       | 0        |

      <style>
        /* couldn't style the table without breaking markdown. quik hax */
        .style-hax + table th:nth-child(n + 3):not(:has(strong))::before,
        .style-hax + table td:nth-child(n + 3):not(:has(strong))::before,
        .style-hax + table th:nth-child(n + 3) strong::before,
        .style-hax + table td:nth-child(n + 3) strong::before {
          content: "‚û° ";
          color: inherit;
        }
        .style-hax + table tr th:nth-child(3),
        .style-hax + table tr td:nth-child(3) strong {
          color: #f88;
        }
        .style-hax + table tr th:nth-child(4),
        .style-hax + table tr td:nth-child(4) strong {
          color: #8ff;
        }
      </style>

      <box-note>Another way to go about this is to keep track of probabilities in the form of rational numbers - separately tracking the numerators and the denominators. Only store the numerator per node, and there is one global denominator which is the sum of all the numerators. This is what I did for my demo implementation.</box-note>

      Thus, after updating the probabilities, the state of the graph tells us that the target is around 90% likely to be in the room *R* and 10% in the far hallway *H0*.
      
      The game AI can simply send the guard to the highest node based on the updated probabilities (this case, the room *R*). It can do this again and again, which will result in a seemingly organic and responsive searching behaviour from the AI guard. No predefined patrol routes needed.

      ---

      In case **(2)** where the guard saw the target, a similar but more drastic approach applies. The node containing the target is assigned **1.0** while *the rest of the nodes in the whole graph* are cleared back to **0.0**. Simply because the target can‚Äôt be in two places at once.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('observe-target.png')"
          alt="Node graph with observed target in node having 1.0"
          spec="300"
          caption="After observation of target" />
      </div>

      <box-note>
        Similar to quantum mechanics, an act of observation collapses the superposition. There seems to be an underlying mathematical truth that spans across Markov chains, quantum mechanics, Bayesian networks, and video game mechanics. :P
      </box-note>

      When the guard loses sight of the target, the **1.0** probability will just spread again like a wave and the cycle of chasing, investigation, and hiding continues.

      It‚Äôs best to just see it in action. Play with the demo in the following section.

      ## Demo

      I implemented this algorithm in JavaScript so you can play with it on this page live. In this implementation, the world is a 2D grid where each tile is a node in the Markov graph.
      
      Click a tile to command the target (the green character ![](demo/target.png)) to move. The blue fog indicates the probabilities of each tile.

      Have fun playing hide and seek!

      <dynamic-patrol-demo :map="`
        ....................
        ....................
        .T##########.#####..
        ..#........#..#..#..
        ..#........#..#..#..
        ...........#..#..#..
        ..#........#####.#..
        ..#..............#..
        ..################..
        .G#........#........
        ###........#.....#..
        ..#######.########..
        ..#....#.........#..
        ..#....#.........#..
        ..#.........###..#..
        ..#....#.........#..
        ..#....#.........#..
        ..##.#############..
        ....................
        ....................
      `"/>

      <box-note>**Tip:** Press `P` to toggle visibility of the probability field. Press `N` to toggle percentages. (Keyboard only)</box-note>

      ## Conclusions

      * Notice that upon losing vision, the guard starts chasing in the direction where you ran away, without the guard actually seeing where you are. Happens especially in narrow passageways.
      * As the chase continues, the path begins branching, and the probability dilutes. The guard gradually transitions from chasing behaviour to a searching behaviour, eventually devolving into a plain patrol.
      * All of these behaviours emerged from the Markov-quantum system (idk what to call it). There‚Äôs no explicit code for chasing, searching, or patrolling.
      * The probability spreading process can be drastically sped-up by implementing Markov chain transitions using a transition matrix and matrix multiplication. Matrix multiplication is [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).
      * The search route quality can be improved significantly. Currently it just sets the tile with the highest potential as the destination with an A* pathfinder, resetting the process whenever the tile becomes invalid.
        - One improvement might involve incorporating the potential field as weights in the pathfinding algorithm itself to generate a more efficient and sweeping route.
        - One could use the potential field itself as tile-by-tile directions for the agents. No time-intensive pathfinder needed. The agents might even behave like hound dogs tracking the target by scent.

      Sadly, the name <text-link href="https://github.com/mxgmn/WaveFunctionCollapse">‚ÄúWave Function Collapse‚Äù</text-link> has already been claimed by a different video game algorithm, so I can‚Äôt give this one a cool quantum name anymore.

      ---

      **Bonus demo! 2 guards.**

      <dynamic-patrol-demo :map="`
        .................#..
        .T...............#..
        ..#..####..#######..
        ..#.....#...........
        ..#.....#...........
        ..#..#..#######..###
        ...G.#..#........#..
        .....#..#........#..
        ######..####..####..
        ..#........#........
        ..#.G......#........
        ..#..#######..####..
        ..............#.....
        ..............#.....
        ######..#######..###
        ..#.....#........#..
        ..#.....#........#..
        ..#..#..####..#..#..
        .....#........#.....
        .....#........#.....
      `"/>

      Special thanks:
      * ![](demo/target.png) Ally Gator as ‚ÄúThe Target‚Äù
      * ![](demo/guard.png) Metal Head as ‚ÄúGuard 1‚Äù
      * ![](demo/guard.png) Rust Bucket as ‚ÄúGuard 2‚Äù
      * [bitbucket/umbraprojekt/mrpas](https://bitbucket.org/umbraprojekt/mrpas) for guards‚Äô vision ![](demo/guard.png)ü™ß![](demo/guard.png)
      * [github/qiao/PathFinding.js](https://github.com/qiao/PathFinding.js) for giving directions üß≠
    </markdown>
  </blog-page>
</html>

<style>
  .small-diagram-row {
    display: flex;
    gap: 12px;
  }
  .small-diagram-row-solo {
    margin-left: 25%;
    margin-right: 25%;
  }
  @media (max-width: 600px) {
    .small-diagram-row {
      margin: 0;
      flex-direction: column;
    }
  }
</style>
