<html lang="en">
  <page-title title="Dynamic patrol behaviour in stealth games with Markov chains" />
  <link
    rel="syndication"
    href="https://www.reddit.com/r/IndieDev/comments/14nu6zf/i_made_a_potentialfield_based_algorithm_for/"
  />
  <blog-page path-group="/notes/">
    <!-- prettier-ignore -->
    <markdown>
      <blog-header
        title="Dynamic patrol behaviour in stealth games with Markov chains"
        :heroimgsrc="url('hero.jpg')" />

      <blog-post-info date="1 Jul 2023" read-mins="9" />

      <tag-row>
        <tag>algo</tag>
        <tag>games</tag>
      </tag-row>

      Hi, this post is about a game AI algorithm for stealth games.
      
      But first, here’s a preview demo! Full demo at the end of this post. In between, I’ll explain the background, the process, and the results!

      <dynamic-patrol-demo :map="`
        ###################
        #....#....#.......#
        #....#....#.......#
        #....#....#.......#
        ##.####.###.......#
        #.................#
        #.................#
        ##.####.###.......#
        #...#....#####..###
        #...#....#####..###
        #...#...G#####..###
        ##########.......##
        ##########.......##
        ##########.......##
        ##########.......##
        ###################
      `"/>

      ## Background

      I enjoy stealth games. However, I felt like the genre has become formulaic. Nowadays, we have standardised light, shadow, and noise mechanics. We almost always get discrete levels of alertness where on one end NPCs have wallhacks while on the other, NPCs have amnesia.

      <blog-media
        :src="url('sc.jpg')"
        alt="Screenshot of Splinter Cell"
        caption="Screenshot of Splinter Cell: Chaos Theory from mobygames.com. This game is good." />

      The most immersion-breaking moment for me was when you get spotted, the subsequent investigation consists solely of staring at the ground where you were last seen. Like when you get spotted at an entrance to a room, guards will just stare at the doorway. *Why not check inside the room?*

      <box-note>Yeah, I know, game designers don’t deem smart game AI “fun”. But an easy predictable game is no fun! Whatever, I just wanted to share this prototype.</box-note>

      What was lacking in stealth game AI is inference - the ability to infer that when a target enters a room, then they subsequently must be inside the room.

      ## A room and a hallway

      As an example, here are a room and a hallway with a doorway in between, modelled as a [graph](https://en.wikipedia.org/wiki/Graph):

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph.png')"
          alt="Node graph representing a room node, a doorway node, and hallway nodes"
          spec="300" />
      </div>

      If we assign a number to each node representing the probability that the target (the player) is there, we can start making inferences of where the target could be at later times.

      Let’s say the target was just seen in room *R*. At that exact moment, there is complete certainty that the target is there, so node *R* will be assigned a probability of **1.0**.
      
      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-r1.png')"
          alt="Node graph representing the room node with 1.0, the doorway node with 0.0, and hallway nodes with 0.0"
          spec="300" />
      </div>

      After that moment however, the certainty fades. Because the target could have exited the room next, or maybe they stayed.
      
      The graph is recalculated to reflect this uncertainty by distributing the probability value of **1.0** from node *R* to each possible choice of node - *D* (exit) and *R* (stay). We don’t know the likeliness of either happening so we can just assume equal chances, giving them **0.5** each.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-r1d1.png')"
          alt="Node graph representing the room node with 0.5, the doorway node with 0.5, and hallway nodes with 0.0"
          spec="300" />
      </div>

      Now there is 50% probability that the target is in the room, and 50% in the doorway. This process is repeated for each node over time to calculate the target’s potential location at any given moment.
      
      Let’s do another iteration. The next one is a bit tricky, but it’s all calculated the same. We just need to calculate the distribution from each node *in parallel*, like so:

      <div class="small-diagram-row">
        <blog-media
          :src="url('room-graph-r1d1t.png')"
          alt="Node graph representing the distribution from previous state"
          spec="300"
          caption="Split the probabilities per node to each possible choice (including staying)." />
        <blog-media
          :src="url('room-graph-r5d5h2.png')"
          alt="Node graph representing the room node with 0.42, the doorway node with 0.42, and the hallway node H1 directly next to the doorway with 0.16"
          spec="300"
          caption="Then sum up the values that arrived in each node." />
      </div>

      **Explanation of above:** The **0.5** at *R* is split into two, giving **0.5 / 2 = 0.25** each to *R* and *D*. Meanwhile, the **0.5** at *D* is split into three, giving **0.5 / 3 ≈ 0.16** each to *R*, *D*, and *H1*. Then node values are added together in a separate step after the split.

      After some time, we will get a picture of where the target is likely to be and a smarter game AI can utilise this to send guards on a more realistic investigation route.
      
      Another iteration and we get this:

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('room-graph-final.png')"
          alt="Node graph representing the room node with 0.35, the doorway node with 0.39, hallway nodes H0 and H2 with 0.04, and H1 with 0.18"
          spec="300"
          caption="The state after some time" />
      </div>

      What I’ve just described is some generalisation of a **[Markov chain](https://en.wikipedia.org/wiki/Markov_chain)**. Well, it’s not exactly accurate to call it that since the Markov chain is just one part of the algorithm. You’ll see why in the next section.

      <box-note><strong>Disclaimer:</strong> I wasn’t thinking about Markov chains while developing this algorithm. The first version back in 2013 was based on crude counting and was more like a potential field. (Btw, it was in [Flash](https://en.wikipedia.org/wiki/Adobe_Flash).) The Markov chain concept that I learned (2023) helped me make the calculations more accurate and the numbers more realistic.</box-note>

      ## Observer effect

      Suppose a guard did come to investigate the nearest highest probability node (the doorway *D*). Coming from the south, the guard just saw the doorway and the immediate hallway in their field of vision - There are two possibilities: Either **(1)** they saw nothing, or **(2)** they saw *the target*.

      In case **(1)** where the guard saw nothing, we need to update the seen nodes according to the guard’s a posteriori observation.

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('observe-none.png')"
          alt="Node graph with observed nodes having 0.0"
          spec="300"
          caption="After observation" />
      </div>

      The nodes that were seen having no target at their locations are forced to a probability of **0.0**, because if you think about it, that makes sense. The remaining nodes are then scaled so that they still add up to a total of **1.0** (This is an invariant in any case).

      To illustrate, here’s a table that details each intermediate step:

<div class="probability-table">

| Node | Prior<br/>probabilities | Values after<br/>observation | Final values<br/>after rescaling |
|------|-------|-------------|----------|
| R    | 0.35  | 0.35        | **0.9**  |
| D    | 0.39  | **0**       | 0        |
| H0   | 0.04  | 0.04        | **0.1**  |
| H1   | 0.18  | **0**       | 0        |
| H2   | 0.04  | **0**       | 0        |

</div>

      <style>
        /* couldn't style the table without breaking markdown. quik hax */
        .probability-table th:nth-child(n + 3):not(:has(strong))::before,
        .probability-table td:nth-child(n + 3):not(:has(strong))::before,
        .probability-table th:nth-child(n + 3) strong::before,
        .probability-table td:nth-child(n + 3) strong::before {
          content: "➡ ";
          color: inherit;
        }
        .probability-table tr th:nth-child(3),
        .probability-table tr td:nth-child(3) strong {
          color: #f88;
        }
        .probability-table tr th:nth-child(4),
        .probability-table tr td:nth-child(4) strong {
          color: #8ff;
        }
      </style>

      After updating the probabilities, the state of the graph tells us that the target is around 90% likely to be in the room *R* and 10% in the far hallway *H0*.
      
      The game AI can simply send the guard to the highest node based on the updated probabilities (this case, the room *R*). It can do this again and again, which will result in a seemingly organic and responsive searching behaviour from the AI guard. No predefined patrol routes needed.

      <box-note>Another way to go about this is to keep track of probabilities in the form of rational numbers - separately tracking the numerators and the denominators. You only need to store the numerator per node, while there is one global denominator, which is the sum of all the numerators. This is what I did for my demo implementation.</box-note>

      ---

      In case **(2)** where the guard saw the target, a similar but more drastic approach applies. The node containing the target is assigned **1.0** while *the rest of the nodes in the whole graph* are cleared back to **0.0**. The target can only be in one place at at a time!

      <div class="small-diagram-row small-diagram-row-solo">
        <blog-media
          :src="url('observe-target.png')"
          alt="Node graph with observed target in node having 1.0"
          spec="300"
          caption="After observation of target" />
      </div>

      It stays that way as long as the guard can see the target. When the guard loses sight of the target again, we just continue the Markov inference and the probability values will spread again like a wave. The cycle of chasing, investigation, and hiding continues.

      <box-note>
        Similar to quantum mechanics, an act of observation collapses the superposition. There seems to be an underlying mathematical truth that spans across Markov chains, quantum mechanics, Bayesian networks, and video game mechanics. :P
      </box-note>

      It’s best to just see it in action. Play with the demo in the following section.

      ## Demo

      I implemented this algorithm in JavaScript so you can play with it right here. In this implementation, the world is a 2D grid where each tile is a node in the Markov graph.
      
      Click a tile to command the target (the green character ![](demo/target.png)) to move. A blue fog will indicate the probabilities of each tile.

      Have fun playing hide and seek!

      <dynamic-patrol-demo :map="`
        ....................
        ....................
        .T##########.#####..
        ..#........#..#..#..
        ..#........#..#..#..
        ...........#..#..#..
        ..#........#####.#..
        ..#..............#..
        ..################..
        .G#........#........
        ###........#.....#..
        ..#######.########..
        ..#....#.........#..
        ..#....#.........#..
        ..#.........###..#..
        ..#....#.........#..
        ..#....#.........#..
        ..##.#############..
        ....................
        ....................
      `"/>

      <box-note>**Tip:** Press `P` to toggle visibility of the probability field. Press `N` to toggle numbers between none, percentage, and log-scale. (Keyboard only)</box-note>

      ## Conclusions

      * Emergent behaviours can be described:
        - Chasing: Upon losing vision, the guard starts chasing in the direction where you ran away (without the guard actually seeing where you are).
        - Searching: As the chase continues, the path begins branching, and the probability dilutes. The guard gradually transitions from chasing behaviour to a searching behaviour.
        - Patrolling: As the probability distribution approaches equilibrium, the guard devolves into a plain patrol.
        - There is a spectrum from chasing to patrolling.
      * The probability spreading process can be drastically sped-up by implementing Markov chain transitions using matrix multiplication with a transition matrix.
        - Matrix multiplication is [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).
      * The search route quality can be improved significantly. Currently it just sets the tile with the highest potential as the destination with an A* pathfinder, resetting the process whenever the tile becomes invalid.
        - One improvement might involve incorporating the potential field as weights in the pathfinding algorithm itself to generate a more efficient and sweeping route.

      Sadly, the name <text-link href="https://github.com/mxgmn/WaveFunctionCollapse">“Wave Function Collapse”</text-link> has already been claimed by a different video game algorithm, so I can’t give this one a cool quantum name anymore.

      ---

      **Bonus demo! 2 guards.**

      <dynamic-patrol-demo :map="`
        .................#..
        .T...............#..
        ..#..####..#######..
        ..#.....#...........
        ..#.....#...........
        ..#..#..#######..###
        ...G.#..#........#..
        .....#..#........#..
        ######..####..####..
        ..#........#........
        ..#.G......#........
        ..#..#######..####..
        ..............#.....
        ..............#.....
        ######..#######..###
        ..#.....#........#..
        ..#.....#........#..
        ..#..#..####..#..#..
        .....#........#.....
        .....#........#.....
      `"/>

      Special thanks:
      * ![](demo/target.png) Ally Gator as “The Target”
      * ![](demo/guard.png) Metal Head as “Guard 1”
      * ![](demo/guard.png) Rust Bucket as “Guard 2”
      * [bitbucket/umbraprojekt/mrpas](https://bitbucket.org/umbraprojekt/mrpas) for guards’ vision ![](demo/guard.png)🪧![](demo/guard.png)
      * [github/qiao/PathFinding.js](https://github.com/qiao/PathFinding.js) for giving directions 🧭

      **Update:** Related stuff I found:
      * [Predicting Pac-Man ghosts with Markov chains [YouTube]](https://youtu.be/eFP0_rkjwlY?t=842)
        - As mentioned above, I had a crude counting-based solution before, but the concept of Markov chains made it more accurate. This video is where I got the idea of applying Markov chains to a grid.
      * [Dynamic Guard Patrol in Stealth Games [academic, PDF]](https://ojs.aaai.org/index.php/AIIDE/article/download/7425/7308/10903)
        - Accompanying video: [YouTube](https://youtu.be/9FyaMM7l2EU)
        - I found this after writing this post. The paper uses a similar potential field idea, but only tracks *staleness*. It behaves similarly to the Markov chain’s equilibrium state, but it cannot produce chasing behaviours.
    </markdown>
  </blog-page>
</html>

<style>
  .small-diagram-row {
    margin: -36px 0;
    display: flex;
    gap: 12px;
  }
  .small-diagram-row * {
    flex: 1 1 50%;
  }
  .small-diagram-row-solo {
    margin-left: 25%;
    margin-right: 25%;
  }
  @media (max-width: 600px) {
    .small-diagram-row {
      margin-left: 0;
      margin-right: 0;
      flex-direction: column;
    }
  }
</style>
