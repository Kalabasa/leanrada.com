<html>
  <page-title title="Dimensions - Part 2: Augmented reality" />
  <blog-page>
    <!-- prettier-ignore -->
    <markdown>
      # Dimensions

      <blog-media
        alt="Video demo"
        src="{url('media/dimensions_3.mp4')}" />

      ## Part 2. Augmented reality

      The idea of including augmented reality into the art piece was wholly inspired by [**AR.js**](https://github.com/jeromeetienne/AR.js), an awesome project that brings fast and easy augmented reality to the web.

      <iframe width="512" height="288" src="https://www.youtube-nocookie.com/embed/0MtvjFg7tik" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      The AR.js demo looked really cool and smooth. I didn‚Äôt end up using that library though.

      With augmented reality, the art piece became an art experience. The planned user flow for the experience went like this:

      1. User sees the piece and notices the **QR code**.
      1. User whips out phone to **scan** the QR code.
      1. Phone is directed to the **app**.
      1. App opens the phone‚Äôs camera to **track** the piece in 3D space.
      1. App superimposes virtual art on the physical art, *augmenting reality*. ‚ú®

      Implementing object tracking (step 4) proved to be difficult. AR.js and others required special **markers** in order to track the 3D scene.

      The prints were already finalized, so I couldn‚Äôt add AR markers on it by then. Plus, the piece already had a QR code slapped on it. Adding any more tags would‚Äôve ruined it.

      I looked around for alternatives like **Tracking.js**, **OpenCV**, and even **TensorFlow**, but ultimately implemented my own **image recognition** algorithm.

      ---

      ## Recognizing the piece

      For this project, I applied a simple image recognition algorithm to determine when the piece has been aligned in front of the camera. [**Image recognition**](https://en.wikipedia.org/wiki/Computer_vision#Recognition) is a computer vision problem of determining whether an image contains some specific object or not.

      <blog-media
        type="bleed"
        alt="Demo video"
        src="{url('media/dimensions_imagerec.mp4')}"
        caption="The app recognizes when the target piece has been aligned." />

      There exists many solutions to this problem, ranging from simple histogram matching to convolutional neural networks. These days everyone just uses neural networks and deep learning if possible.

      These technologies power some apps like face filters, but are also used for things like mass camera surveillance.

      In my case however, I‚Äôve simplified the problem to determining whether any of the three specific art pieces is in the center frame in the user‚Äôs camera, or not. No position tracking.

      This reduced the problem to a yes/no problem.

      Consequently, the algorithm was relatively simple. It‚Äôs just a color normalization step and a straightforward ‚Äúfeature matching‚Äù of the target images.

      <box-note>I wrote about the algorithm in greater detail in my [blog post](/blog/simple-image-recognition-vanilla-js/)!</box-note>

      Once it gets a match the **augmented reality** experience starts rolling in.

      ---

      ## Augmenting reality

      The vision for the augmented reality part was that the tiles in the piece would come alive, burst out of the piece, and start drawing streaks of paint, ink, or whatever in the air, depending on the tile type.

      <blog-media
        alt="AR notes"
        src="{url('media/dimensions_arnotes.jpg')}" />

      It was basically going to be a particle system.

      This was rendered using [**three.js**](https://threejs.org/). I‚Äôve used three.js before and it was great, with easy-to-learn APIs and good examples.

      I was quickly able to sketch out virtual objects in space as a prototype.

      <blog-media
        alt="Prototype video"
        src="{url('media/dimensions_ar1.mp4')}" />

      This was made by overlaying a transparent three.js `&lt;canvas&gt;` onto the `&lt;video&gt;` that‚Äôs streaming the camera feed.

      A three.js extension called `DeviceOrientationControls` provides synchronization between the device‚Äôs orientation and the virtual camera.

      One caveat though is that only the orientation can be tracked. Tracking movements across space weren‚Äôt possible yet, so virtual objects would appear follow the device when it moves.

      The experience was designed around this limitation by keeping the objects at constant distance to the user, subtly hinting that there‚Äôs no need to move or walk, only looking around.

      <small class="small-block">There was a bug on iOS Safari with orientation tracking, which apparently was just introduced July 2019, one month before the event. Sadly some iPhone users did not get the full experience.</small>

      Modeling the tiles as 3D objects were simply extrusions of the tiles‚Äô 2D shape paths, made very easy using three.js‚Äôs `ExtrudeGeometry`.

      The ‚Äúpaint‚Äù trails were made using an old unmaintained library called [**TrailRendererJS**](https://github.com/mkkellogg/TrailRendererJS), which surprisingly still works, although it bugs out when the virtual camera isn‚Äôt at the origin.

      <blog-media
        alt="TrailRenderer example"
        src="{url('media/dimensions_trailex.png')}" />

      The floating tiles‚Äô movement behavior were guided by a smooth triangle wave function:

      <blog-media
        alt="smooth triangle wave function"
        src="{url('media/dimensions_trianglewave2.png')}"
        caption="y = arccos(0.95 sin(x))" />

      This triangle wave path was wrapped around a virtual cylinder around the user‚Äôs position.

      With each floating tile following a variant of this path, the result was an organized chaos of criss-crossing particles orbiting the user.

      <blog-media
        alt="Demo video"
        src="{url('media/dimensions_trail.mp4')}" />

      One interesting experiment was when the trails were allowed to go on indefinitely. The trails would eventually paint the whole scene, producing a nice pattern.

      <blog-media
        src="{url('media/dimensions_trailart.png')}"
        caption="Trail art" />

      When it finally exhibited, I mostly watched from the sidelines, taking notes on how people interacted. It‚Äôs like live testing on prod.

      There were some UX issues that had to be fixed. It wasn‚Äôt as seamless as I‚Äôve hoped, and explicit instructions on how to use it were needed.

      It went smoothly for the most part, after a few hotfixes. Seeing people react to it was great! ‚≠êÔ∏è

      The iOS bug was never fixed. ü§ñ

      <blog-media
        alt="Reaction video"
        src="{url('media/dimensions_2.mp4')}" />

      You can give it a try right on this page!

      Simply open this page on a desktop, and then point your phone camera to the following image, assuming your camera app has a QR code scanning feature.

      <blog-media
        type="bleed"
        src="{url('media/dimensions_finalset.jpg')}"
        caption="Go to kalabasa.github.io/dimensions/ on phone if QR doesn‚Äôt work." />

      It has been a really fun and challenging project! ‚≠ê
    </markdown>
  </blog-page>
</html>
